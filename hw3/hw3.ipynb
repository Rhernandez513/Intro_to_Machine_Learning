{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Multiclass and Linear Models\n",
    "\n",
    "UIC CS 412, Spring 2021\n",
    "\n",
    "*According to the Academic Integrity Policy of this course, all work submitted for grading must be done individually.  While we encourage you to talk to your peers and learn from them, this interaction must be superficial with regards to all work submitted for grading.  This means you cannot work in teams, you cannot work side-by-side, you cannot submit someone elseâ€™s work (partial or complete) as your own. In particular, note that you are guilty of academic dishonesty if you extend or receive any kind of unauthorized assistance.  Absolutely no transfer of program code between students is permitted (paper or electronic), and you may not solicit code from family, friends, or online forums.  Other examples of academic dishonesty include emailing your program to another student, copying-pasting code from the internet, working in a group on a homework assignment, and allowing a tutor, TA, or another individual to write an answer for you.  Academic dishonesty is unacceptable, and penalties range from failure to expulsion from the university; cases are handled via the official student conduct process described at https://dos.uic.edu/conductforstudents.shtml.* \n",
    "\n",
    "This homework is an individual assignment for all graduate students. Undergraduate students are allowed to work in pairs and submit one homework assignment per pair. There will be no extra credit given to undergraduate students who choose to work alone. The pairs of students who choose to work together and submit one homework assignment together still need to abide by the Academic Integrity Policy and not share or receive help from others (except each other).\n",
    "\n",
    "There are two parts to this project. The first is on multiclass reductions (50%). The second is on linear models and gradient descent (50%). There are also opportunities for extra credit (up to 25%).\n",
    "\n",
    "\n",
    "## Due Date\n",
    "\n",
    "This assignment is due at 11:59pm Thursday, March 11th. \n",
    "\n",
    "### Files You'll Edit\n",
    "\n",
    "``multiclass.py``: The multiclass classification implementation you need to complete.\n",
    "\n",
    "``gd.py``: The gradient descent file you need to edit.\n",
    "\n",
    "``quizbowl.py``: Multiclass evaluation of the quiz bowl dataset (optional).\n",
    "\n",
    "``predictions.txt``: This file is automatically generated as part of Part 3 (optional).\n",
    "\n",
    "### Files you might want to look at\n",
    "  \n",
    "``binary.py``: Our generic interface for binary classifiers (actually\n",
    "works for regression and other types of classification, too).\n",
    "\n",
    "``datasets.py``: Where a handful of test data sets are stored.\n",
    "\n",
    "``util.py``: A handful of useful utility functions: these will\n",
    "undoubtedly be helpful to you, so take a look!\n",
    "\n",
    "``runClassifier.py``: A few wrappers for doing useful things with\n",
    "classifiers, like training them, generating learning curves, etc.\n",
    "\n",
    "``mlGraphics.py``: A few useful plotting commands\n",
    "\n",
    "``data/*``: All of the datasets we'll use.\n",
    "\n",
    "### What to Submit\n",
    "\n",
    "You will hand in all of the python files listed above as a single zip file **hw3.zip** on Gradescope under *Homework 3*. In order for the autograder to run, make sure the zip file contains **all** the .py and .ipynb files, not a folder with these files. The programming part constitutes 60% of the grade for this homework. You also need to answer the questions denoted by **WU#** (and a kitten) in this notebook which are the other 40% of your homework grade. There are also up to 25% of extra credit questions denoted by **WU-EC1**, **WU-EC3**, and **WU-EC3**. When you are done, you should export **hw3.ipynb** with your answers as a PDF file **hw3WrittenPart.pdf**, upload the PDF file to Gradescope under *Homework 3 - Written Part*, and **tag each question** on Gradescope. Questions that are not tagged will not be graded.\n",
    "\n",
    "The simplest and recommended way to export your python notebook is to select File -> Print Preview (this appears on the notebook, right under the Jupyter logo), then use the browser to print as PDF (e.g., on Chrome this appears under File->Print...->Destination \"Save as PDF\"). Make sure you double check your final PDF to make sure it's not missing any pieces before submitting your final version!\n",
    "\n",
    "Your entire homework will be considered late if any of these parts are submitted late. \n",
    "\n",
    "#### Autograding\n",
    "\n",
    "Your code will be autograded for technical correctness. Please **do\n",
    "not** change the names of any provided functions or classes within the\n",
    "code, or you will wreak havoc on the autograder. We have provided two simple test cases that you can try your code on, see ``run_tests_sample.py``. As usual, you should create more test cases to make sure your code runs correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Autoreload "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, let's import a jupyter notebook extension called [`autoreload`](https://ipython.org/ipython-doc/3/config/extensions/autoreload.html) which would automatically reload changes to external files that you edit.\n",
    "If you change something in a file and the changes are not reflected even after an autoreload, you may have to restart your jupyter notebook kernel (Kernel -> Restart).\n",
    "\n",
    "A manual alternative to `autoreload` is to reload a particular file using `importlib`.\n",
    "\n",
    "``import importlib\n",
    "importlib.reload(dumbClassifiers)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Multiclass Classification *[30% impl, 20% writeup]*\n",
    "\n",
    "In this section, you will explore the differences between three\n",
    "multiclass-to-binary reductions: one-versus-all (OVA), all-versus-all\n",
    "(AVA), and a tree-based reduction (TREE).  The evaluation will be on different datasets from \n",
    "`datasets.py`.\n",
    "\n",
    "The classification task we'll work with is wine classification. The dataset was downloaded from allwines.com. Your job is to predict the type of wine, given the description of the wine. There are two tasks: WineData has **20** different wines, WineDataSmall is just the first five of those (sorted roughly by frequency). You can find the names of the wines both in WineData.labels as well as the file wines.names.\n",
    "\n",
    "To start out, let's import everything and train decision \"stumps\" (aka depth=1 decision trees) on the large data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training classifier for 0 versus rest\n",
      "training classifier for 1 versus rest\n",
      "training classifier for 2 versus rest\n",
      "training classifier for 3 versus rest\n",
      "training classifier for 4 versus rest\n",
      "training classifier for 5 versus rest\n",
      "training classifier for 6 versus rest\n",
      "training classifier for 7 versus rest\n",
      "training classifier for 8 versus rest\n",
      "training classifier for 9 versus rest\n",
      "training classifier for 10 versus rest\n",
      "training classifier for 11 versus rest\n",
      "training classifier for 12 versus rest\n",
      "training classifier for 13 versus rest\n",
      "training classifier for 14 versus rest\n",
      "training classifier for 15 versus rest\n",
      "training classifier for 16 versus rest\n",
      "training classifier for 17 versus rest\n",
      "training classifier for 18 versus rest\n",
      "training classifier for 19 versus rest\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2949907235621521"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import multiclass\n",
    "import util\n",
    "from datasets import *\n",
    "import importlib\n",
    "from pylab import *\n",
    "\n",
    "h = multiclass.OVA(20, lambda: DecisionTreeClassifier(max_depth=1))\n",
    "h.train(WineData.X, WineData.Y)\n",
    "P = h.predictAll(WineData.Xte)\n",
    "mean(P == WineData.Yte)\n",
    "# 0.29499072356215211"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means 29% accuracy on this task. The most frequent class is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Cabernet-Sauvignon'"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(mode(WineData.Y))\n",
    "# 1\n",
    "WineData.labels[1]\n",
    "# Cabernet-Sauvignon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you were to always predict label 1, you would get the following accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1725417439703154"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(WineData.Yte == 1)\n",
    "# 0.17254174397031541"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So decision stumps producing a bit (12%) better accuracy than just predicting label 1. \n",
    "\n",
    "The default implementation of OVA uses decision tree confidence (probability of prediction) to weigh the votes. You can switch to zero/one predictions to see the effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19109461966604824"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = h.predictAll(WineData.Xte, useZeroOne=True)\n",
    "mean(P == WineData.Yte)\n",
    "# 0.19109461966604824"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this is markedly worse.\n",
    "\n",
    "Switching to the smaller data set for a minute, we can train, say, depth 3 decision trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training classifier for 0 versus rest\n",
      "training classifier for 1 versus rest\n",
      "training classifier for 2 versus rest\n",
      "training classifier for 3 versus rest\n",
      "training classifier for 4 versus rest\n",
      "0.6083150984682714\n",
      "0.40700218818380746\n"
     ]
    }
   ],
   "source": [
    "h = multiclass.OVA(5, lambda: DecisionTreeClassifier(max_depth=3))\n",
    "h.train(WineDataSmall.X, WineDataSmall.Y)\n",
    "P = h.predictAll(WineDataSmall.Xte)\n",
    "print(mean(P == WineDataSmall.Yte))\n",
    "# 0.6017505470459519\n",
    "print(mean(WineDataSmall.Yte == 1))\n",
    "# 0.407002188184"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So using depth 3 trees we get an accuracy of about 60% (this number varies a bit), versus a baseline of 41%. That's not too terrible, but not great.\n",
    "\n",
    "We can look at what this classifier is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sauvignon-Blanc\n",
      "citrus?\n",
      "-N-> lime?\n",
      "|    -N-> gooseberry?\n",
      "|    |    -N-> class -1\t(356.0 for class -1, 10.0 for class +1)\n",
      "|    |    -Y-> class +1\t(0.0 for class -1, 4.0 for class +1)\n",
      "|    -Y-> or?\n",
      "|    |    -N-> class +1\t(1.0 for class -1, 15.0 for class +1)\n",
      "|    |    -Y-> class -1\t(2.0 for class -1, 0.0 for class +1)\n",
      "-Y-> grapefruit?\n",
      "|    -N-> flavors?\n",
      "|    |    -N-> class +1\t(4.0 for class -1, 12.0 for class +1)\n",
      "|    |    -Y-> class -1\t(11.0 for class -1, 5.0 for class +1)\n",
      "|    -Y-> opens?\n",
      "|    |    -N-> class +1\t(0.0 for class -1, 14.0 for class +1)\n",
      "|    |    -Y-> class -1\t(1.0 for class -1, 0.0 for class +1)\n"
     ]
    }
   ],
   "source": [
    "print(WineDataSmall.labels[0])\n",
    "#'Sauvignon-Blanc'\n",
    "util.showTree(h.f[0], WineDataSmall.words)\n",
    "#citrus?\n",
    "#-N-> lime?\n",
    "#|    -N-> gooseberry?\n",
    "#|    |    -N-> class 0\t(356.0 for class 0, 10.0 for class 1)\n",
    "#|    |    -Y-> class 1\t(0.0 for class 0, 4.0 for class 1)\n",
    "#|    -Y-> apple?\n",
    "#|    |    -N-> class 1\t(1.0 for class 0, 15.0 for class 1)\n",
    "#|    |    -Y-> class 0\t(2.0 for class 0, 0.0 for class 1)\n",
    "#-Y-> grapefruit?\n",
    "#|    -N-> flavors?\n",
    "#|    |    -N-> class 1\t(4.0 for class 0, 12.0 for class 1)\n",
    "#|    |    -Y-> class 0\t(11.0 for class 0, 5.0 for class 1)\n",
    "#|    -Y-> opens?\n",
    "#|    |    -N-> class 1\t(0.0 for class 0, 14.0 for class 1)\n",
    "#|    |    -Y-> class 0\t(1.0 for class 0, 0.0 for class 1)\n",
    "\n",
    "# It's okay to get a slightly different tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should show the tree that's associated with predicting label 0 (which is stored in h.f[0]). The 1s mean \"likely to be Sauvignon-Blanc\" and the 0s mean \"likely not to be\".\n",
    "\n",
    "Now, go in and complete the AVA implementation in `multiclass.py`. Make sure you follow the corrected version of the algorithm from the class slides, not the one from the book. You should be able to train an AVA model on the small data set by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training classifier for 1 versus 0\n",
      "training classifier for 2 versus 0\n",
      "training classifier for 2 versus 1\n",
      "training classifier for 3 versus 0\n",
      "training classifier for 3 versus 1\n",
      "training classifier for 3 versus 2\n",
      "training classifier for 4 versus 0\n",
      "training classifier for 4 versus 1\n",
      "training classifier for 4 versus 2\n",
      "training classifier for 4 versus 3\n",
      "0.05689277899343545\n",
      "citrus?\n",
      "-N-> lime?\n",
      "|    -N-> refreshing?\n",
      "|    |    -N-> class -1\t(187.0 for class -1, 9.0 for class +1)\n",
      "|    |    -Y-> class +1\t(0.0 for class -1, 5.0 for class +1)\n",
      "|    -Y-> class +1\t(0.0 for class -1, 15.0 for class +1)\n",
      "-Y-> class +1\t(0.0 for class -1, 31.0 for class +1)\n"
     ]
    }
   ],
   "source": [
    "h = multiclass.AVA(5, lambda: DecisionTreeClassifier(max_depth=3))\n",
    "h.train(WineDataSmall.X, WineDataSmall.Y)\n",
    "P = h.predictAll(WineDataSmall.Xte)\n",
    "print(mean(P == WineDataSmall.Yte))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you must implement a \n",
    "tree-based reduction MCTree in `multiclass.py`. Most of train is given to you, but predict you\n",
    "must do all on your own. There is a `makeBalancedTree` function to help you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1] [2 [3 4]]]\n",
      "False\n",
      "[2 [3 4]]\n",
      "2\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "t = multiclass.makeBalancedTree(range(5))\n",
    "print(t)\n",
    "# [[0 1]] [2 [3 4]]]\n",
    "print(t.isLeaf)\n",
    "# False\n",
    "print(t.getRight())\n",
    "# [2 [3 4]]\n",
    "print(t.getRight().getLeft())\n",
    "# 2\n",
    "print(t.getRight().getLeft().isLeaf)\n",
    "# True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to train a MCTree model by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training classifier for [0, 1] versus [2, 3, 4]\n",
      "training classifier for [0] versus [1]\n",
      "training classifier for [2] versus [3, 4]\n",
      "training classifier for [3] versus [4]\n"
     ]
    }
   ],
   "source": [
    "h = multiclass.MCTree(t, lambda: DecisionTreeClassifier(max_depth=3))\n",
    "h.train(WineDataSmall.X, WineDataSmall.Y)\n",
    "P = h.predictAll(WineDataSmall.Xte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training classifier for [0, 1] versus [2, 3, 4]\n",
      "training classifier for [0] versus [1]\n",
      "training classifier for [2] versus [3, 4]\n",
      "training classifier for [3] versus [4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07221006564551423"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = multiclass.makeBalancedTree(range(5))\n",
    "h = multiclass.MCTree(t, lambda: DecisionTreeClassifier(max_depth=3))\n",
    "h.train(WineDataSmall.X, WineDataSmall.Y)\n",
    "P = h.predictAll(WineDataSmall.Xte)\n",
    "np.mean(P == WineDataSmall.Yte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/kitten.jpeg\" width=\"100px\" align=\"left\" float=\"left\"/>\n",
    "<br><br><br>\n",
    "\n",
    "## WU1 (10%):\n",
    "Answer A, B, C for both OVA and AVA.\n",
    "\n",
    "(A) Using WineDataSmall, answer the following: What words are most indicative of being Sauvignon-Blanc? Which words are most indicative of not being Cabernet-Sauvignon? What about Pinot-Grigio (label=4)? Note that indicative words describe a specific wine (not others).\n",
    "\n",
    "(B) Train depth 3 decision trees on the full WineData task (with 20 labels). What accuracy do you get? How long does this take (in seconds)? One of my least favorite wines is Shiraz -- what words are indicative of this?\n",
    "\n",
    "(C) Compare the accuracy using zero-one predictions versus using confidence on the full WineData. How much difference does it make?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A using OVA\n",
      "training classifier for 0 versus rest\n",
      "training classifier for 1 versus rest\n",
      "training classifier for 2 versus rest\n",
      "training classifier for 3 versus rest\n",
      "training classifier for 4 versus rest\n",
      "\n",
      "Sauvignon-Blanc\n",
      "citrus?\n",
      "-N-> lime?\n",
      "|    -N-> gooseberry?\n",
      "|    |    -N-> class -1\t(356.0 for class -1, 10.0 for class +1)\n",
      "|    |    -Y-> class +1\t(0.0 for class -1, 4.0 for class +1)\n",
      "|    -Y-> apple?\n",
      "|    |    -N-> class +1\t(1.0 for class -1, 15.0 for class +1)\n",
      "|    |    -Y-> class -1\t(2.0 for class -1, 0.0 for class +1)\n",
      "-Y-> grapefruit?\n",
      "|    -N-> flavors?\n",
      "|    |    -N-> class +1\t(4.0 for class -1, 12.0 for class +1)\n",
      "|    |    -Y-> class -1\t(11.0 for class -1, 5.0 for class +1)\n",
      "|    -Y-> 2010?\n",
      "|    |    -N-> class +1\t(0.0 for class -1, 14.0 for class +1)\n",
      "|    |    -Y-> class -1\t(1.0 for class -1, 0.0 for class +1)\n",
      "\"citrus\" and \"grapefruit\" both being present combined with a lack of \"honey\"\n",
      "are the most indicative of Sauvignon-Blanc\n",
      "\n",
      "Cabernet-Sauvignon\n",
      "tannins?\n",
      "-N-> blackberry?\n",
      "|    -N-> black?\n",
      "|    |    -N-> class -1\t(173.0 for class -1, 30.0 for class +1)\n",
      "|    |    -Y-> class +1\t(24.0 for class -1, 28.0 for class +1)\n",
      "|    -Y-> mushroom?\n",
      "|    |    -N-> class +1\t(4.0 for class -1, 22.0 for class +1)\n",
      "|    |    -Y-> class -1\t(2.0 for class -1, 0.0 for class +1)\n",
      "-Y-> salmon?\n",
      "|    -N-> raspberries?\n",
      "|    |    -N-> class +1\t(29.0 for class -1, 106.0 for class +1)\n",
      "|    |    -Y-> class -1\t(8.0 for class -1, 1.0 for class +1)\n",
      "|    -Y-> class -1\t(8.0 for class -1, 0.0 for class +1)\n",
      "\"tannins\" and \"salmon\" are most indicative of not being Cabernet-Sauvignon\n",
      "\n",
      "Pinot-Grigio\n",
      "apple?\n",
      "-N-> paired?\n",
      "|    -N-> friends?\n",
      "|    |    -N-> class -1\t(395.0 for class -1, 13.0 for class +1)\n",
      "|    |    -Y-> class +1\t(1.0 for class -1, 2.0 for class +1)\n",
      "|    -Y-> sweet?\n",
      "|    |    -N-> class +1\t(0.0 for class -1, 3.0 for class +1)\n",
      "|    |    -Y-> class -1\t(2.0 for class -1, 0.0 for class +1)\n",
      "-Y-> straw?\n",
      "|    -N-> dishes?\n",
      "|    |    -N-> class -1\t(7.0 for class -1, 2.0 for class +1)\n",
      "|    |    -Y-> class +1\t(0.0 for class -1, 2.0 for class +1)\n",
      "|    -Y-> length?\n",
      "|    |    -N-> class +1\t(0.0 for class -1, 7.0 for class +1)\n",
      "|    |    -Y-> class -1\t(1.0 for class -1, 0.0 for class +1)\n",
      "\"apple\" \"straw\" and \"bright\" are most indicative of Pinot-Grigio\n",
      "while the absense of \"apple\" \"paired\" and \"friends\" are most indicative of\n",
      "not being Pinot-Grigio\n",
      "\n",
      "\n",
      "B using OVA\n",
      "training classifier for 0 versus rest\n",
      "training classifier for 1 versus rest\n",
      "training classifier for 2 versus rest\n",
      "training classifier for 3 versus rest\n",
      "training classifier for 4 versus rest\n",
      "training classifier for 5 versus rest\n",
      "training classifier for 6 versus rest\n",
      "training classifier for 7 versus rest\n",
      "training classifier for 8 versus rest\n",
      "training classifier for 9 versus rest\n",
      "training classifier for 10 versus rest\n",
      "training classifier for 11 versus rest\n",
      "training classifier for 12 versus rest\n",
      "training classifier for 13 versus rest\n",
      "training classifier for 14 versus rest\n",
      "training classifier for 15 versus rest\n",
      "training classifier for 16 versus rest\n",
      "training classifier for 17 versus rest\n",
      "training classifier for 18 versus rest\n",
      "training classifier for 19 versus rest\n",
      "\n",
      "Full Wine Data confidence: 0.3673469387755102\n",
      "--- 1.5885069370269775 seconds ---\n",
      "characters?\n",
      "-N-> barossa?\n",
      "|    -N-> brooding?\n",
      "|    |    -N-> class -1\t(1004.0 for class -1, 23.0 for class +1)\n",
      "|    |    -Y-> class +1\t(3.0 for class -1, 5.0 for class +1)\n",
      "|    -Y-> yellow?\n",
      "|    |    -N-> class +1\t(0.0 for class -1, 4.0 for class +1)\n",
      "|    |    -Y-> class -1\t(1.0 for class -1, 0.0 for class +1)\n",
      "-Y-> dark?\n",
      "|    -N-> spice?\n",
      "|    |    -N-> class -1\t(18.0 for class -1, 1.0 for class +1)\n",
      "|    |    -Y-> class +1\t(0.0 for class -1, 3.0 for class +1)\n",
      "|    -Y-> it?\n",
      "|    |    -N-> class +1\t(1.0 for class -1, 10.0 for class +1)\n",
      "|    |    -Y-> class -1\t(3.0 for class -1, 1.0 for class +1)\n",
      "\n",
      "We get a YTest accuracry of 37% (0.36641929499072357)\n",
      "It takes about 1.6 seconds to accomplish this\n",
      "\"characters\" \"dark\" and the absence of \"it\" are most indicative of being Shiraz\n",
      "\"barossa\" and \"brooding\" are most indicative of not being Shiraz\n",
      "\n",
      "\n",
      "C using OVA\n",
      "\n",
      "Full Wine Data zeroOne: 0.24582560296846012\n",
      "without zeroOne confidence = 0.3682745825602968 while zeroOne = 0.2504638218923933\n",
      "Just about a 10% difference in accuracy, which is sizeable\n",
      "\n",
      "\n",
      "A using AVA\n",
      "training classifier for 1 versus 0\n",
      "training classifier for 2 versus 0\n",
      "training classifier for 2 versus 1\n",
      "training classifier for 3 versus 0\n",
      "training classifier for 3 versus 1\n",
      "training classifier for 3 versus 2\n",
      "training classifier for 4 versus 0\n",
      "training classifier for 4 versus 1\n",
      "training classifier for 4 versus 2\n",
      "training classifier for 4 versus 3\n",
      "\n",
      "Sauvignon-Blanc\n",
      "citrus?\n",
      "-N-> lime?\n",
      "|    -N-> melon?\n",
      "|    |    -N-> class -1\t(187.0 for class -1, 9.0 for class +1)\n",
      "|    |    -Y-> class +1\t(0.0 for class -1, 5.0 for class +1)\n",
      "|    -Y-> class +1\t(0.0 for class -1, 15.0 for class +1)\n",
      "-Y-> class +1\t(0.0 for class -1, 31.0 for class +1)\n",
      "\n",
      "\"Citrus\" is by far the most indicative of being Sauvignon-Blanc\n",
      "\n",
      "Cabernet-Sauvignon\n",
      "crisp?\n",
      "-N-> lime?\n",
      "|    -N-> lemon?\n",
      "|    |    -N-> class -1\t(141.0 for class -1, 9.0 for class +1)\n",
      "|    |    -Y-> class +1\t(0.0 for class -1, 8.0 for class +1)\n",
      "|    -Y-> roasted?\n",
      "|    |    -N-> class +1\t(0.0 for class -1, 13.0 for class +1)\n",
      "|    |    -Y-> class -1\t(1.0 for class -1, 0.0 for class +1)\n",
      "-Y-> red?\n",
      "|    -N-> class +1\t(0.0 for class -1, 30.0 for class +1)\n",
      "|    -Y-> class -1\t(2.0 for class -1, 0.0 for class +1)\n",
      "\"crip\" \"red\" are most indicative of not being Cabernet-Sauvignon\n",
      "\n",
      "Pinot-Grigio\n",
      "apple?\n",
      "-N-> pasta?\n",
      "|    -N-> quite?\n",
      "|    |    -N-> class +1\t(11.0 for class -1, 56.0 for class +1)\n",
      "|    |    -Y-> class -1\t(3.0 for class -1, 0.0 for class +1)\n",
      "|    -Y-> class -1\t(4.0 for class -1, 0.0 for class +1)\n",
      "-Y-> bright?\n",
      "|    -N-> class -1\t(10.0 for class -1, 0.0 for class +1)\n",
      "|    -Y-> fruits?\n",
      "|    |    -N-> class +1\t(0.0 for class -1, 4.0 for class +1)\n",
      "|    |    -Y-> class -1\t(1.0 for class -1, 0.0 for class +1)\n",
      "\"apple\" \"paste\" \"warm\" all being absent are most indicative of being Pinot-Grigio\n",
      "\"apple\" being present and \"bright\" being absent are most indicative of not being Pinot-Grigio\n",
      "\n",
      "\n",
      "B using AVA\n",
      "training classifier for 1 versus 0\n",
      "training classifier for 2 versus 0\n",
      "training classifier for 2 versus 1\n",
      "training classifier for 3 versus 0\n",
      "training classifier for 3 versus 1\n",
      "training classifier for 3 versus 2\n",
      "training classifier for 4 versus 0\n",
      "training classifier for 4 versus 1\n",
      "training classifier for 4 versus 2\n",
      "training classifier for 4 versus 3\n",
      "training classifier for 5 versus 0\n",
      "training classifier for 5 versus 1\n",
      "training classifier for 5 versus 2\n",
      "training classifier for 5 versus 3\n",
      "training classifier for 5 versus 4\n",
      "training classifier for 6 versus 0\n",
      "training classifier for 6 versus 1\n",
      "training classifier for 6 versus 2\n",
      "training classifier for 6 versus 3\n",
      "training classifier for 6 versus 4\n",
      "training classifier for 6 versus 5\n",
      "training classifier for 7 versus 0\n",
      "training classifier for 7 versus 1\n",
      "training classifier for 7 versus 2\n",
      "training classifier for 7 versus 3\n",
      "training classifier for 7 versus 4\n",
      "training classifier for 7 versus 5\n",
      "training classifier for 7 versus 6\n",
      "training classifier for 8 versus 0\n",
      "training classifier for 8 versus 1\n",
      "training classifier for 8 versus 2\n",
      "training classifier for 8 versus 3\n",
      "training classifier for 8 versus 4\n",
      "training classifier for 8 versus 5\n",
      "training classifier for 8 versus 6\n",
      "training classifier for 8 versus 7\n",
      "training classifier for 9 versus 0\n",
      "training classifier for 9 versus 1\n",
      "training classifier for 9 versus 2\n",
      "training classifier for 9 versus 3\n",
      "training classifier for 9 versus 4\n",
      "training classifier for 9 versus 5\n",
      "training classifier for 9 versus 6\n",
      "training classifier for 9 versus 7\n",
      "training classifier for 9 versus 8\n",
      "training classifier for 10 versus 0\n",
      "training classifier for 10 versus 1\n",
      "training classifier for 10 versus 2\n",
      "training classifier for 10 versus 3\n",
      "training classifier for 10 versus 4\n",
      "training classifier for 10 versus 5\n",
      "training classifier for 10 versus 6\n",
      "training classifier for 10 versus 7\n",
      "training classifier for 10 versus 8\n",
      "training classifier for 10 versus 9\n",
      "training classifier for 11 versus 0\n",
      "training classifier for 11 versus 1\n",
      "training classifier for 11 versus 2\n",
      "training classifier for 11 versus 3\n",
      "training classifier for 11 versus 4\n",
      "training classifier for 11 versus 5\n",
      "training classifier for 11 versus 6\n",
      "training classifier for 11 versus 7\n",
      "training classifier for 11 versus 8\n",
      "training classifier for 11 versus 9\n",
      "training classifier for 11 versus 10\n",
      "training classifier for 12 versus 0\n",
      "training classifier for 12 versus 1\n",
      "training classifier for 12 versus 2\n",
      "training classifier for 12 versus 3\n",
      "training classifier for 12 versus 4\n",
      "training classifier for 12 versus 5\n",
      "training classifier for 12 versus 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training classifier for 12 versus 7\n",
      "training classifier for 12 versus 8\n",
      "training classifier for 12 versus 9\n",
      "training classifier for 12 versus 10\n",
      "training classifier for 12 versus 11\n",
      "training classifier for 13 versus 0\n",
      "training classifier for 13 versus 1\n",
      "training classifier for 13 versus 2\n",
      "training classifier for 13 versus 3\n",
      "training classifier for 13 versus 4\n",
      "training classifier for 13 versus 5\n",
      "training classifier for 13 versus 6\n",
      "training classifier for 13 versus 7\n",
      "training classifier for 13 versus 8\n",
      "training classifier for 13 versus 9\n",
      "training classifier for 13 versus 10\n",
      "training classifier for 13 versus 11\n",
      "training classifier for 13 versus 12\n",
      "training classifier for 14 versus 0\n",
      "training classifier for 14 versus 1\n",
      "training classifier for 14 versus 2\n",
      "training classifier for 14 versus 3\n",
      "training classifier for 14 versus 4\n",
      "training classifier for 14 versus 5\n",
      "training classifier for 14 versus 6\n",
      "training classifier for 14 versus 7\n",
      "training classifier for 14 versus 8\n",
      "training classifier for 14 versus 9\n",
      "training classifier for 14 versus 10\n",
      "training classifier for 14 versus 11\n",
      "training classifier for 14 versus 12\n",
      "training classifier for 14 versus 13\n",
      "training classifier for 15 versus 0\n",
      "training classifier for 15 versus 1\n",
      "training classifier for 15 versus 2\n",
      "training classifier for 15 versus 3\n",
      "training classifier for 15 versus 4\n",
      "training classifier for 15 versus 5\n",
      "training classifier for 15 versus 6\n",
      "training classifier for 15 versus 7\n",
      "training classifier for 15 versus 8\n",
      "training classifier for 15 versus 9\n",
      "training classifier for 15 versus 10\n",
      "training classifier for 15 versus 11\n",
      "training classifier for 15 versus 12\n",
      "training classifier for 15 versus 13\n",
      "training classifier for 15 versus 14\n",
      "training classifier for 16 versus 0\n",
      "training classifier for 16 versus 1\n",
      "training classifier for 16 versus 2\n",
      "training classifier for 16 versus 3\n",
      "training classifier for 16 versus 4\n",
      "training classifier for 16 versus 5\n",
      "training classifier for 16 versus 6\n",
      "training classifier for 16 versus 7\n",
      "training classifier for 16 versus 8\n",
      "training classifier for 16 versus 9\n",
      "training classifier for 16 versus 10\n",
      "training classifier for 16 versus 11\n",
      "training classifier for 16 versus 12\n",
      "training classifier for 16 versus 13\n",
      "training classifier for 16 versus 14\n",
      "training classifier for 16 versus 15\n",
      "training classifier for 17 versus 0\n",
      "training classifier for 17 versus 1\n",
      "training classifier for 17 versus 2\n",
      "training classifier for 17 versus 3\n",
      "training classifier for 17 versus 4\n",
      "training classifier for 17 versus 5\n",
      "training classifier for 17 versus 6\n",
      "training classifier for 17 versus 7\n",
      "training classifier for 17 versus 8\n",
      "training classifier for 17 versus 9\n",
      "training classifier for 17 versus 10\n",
      "training classifier for 17 versus 11\n",
      "training classifier for 17 versus 12\n",
      "training classifier for 17 versus 13\n",
      "training classifier for 17 versus 14\n",
      "training classifier for 17 versus 15\n",
      "training classifier for 17 versus 16\n",
      "training classifier for 18 versus 0\n",
      "training classifier for 18 versus 1\n",
      "training classifier for 18 versus 2\n",
      "training classifier for 18 versus 3\n",
      "training classifier for 18 versus 4\n",
      "training classifier for 18 versus 5\n",
      "training classifier for 18 versus 6\n",
      "training classifier for 18 versus 7\n",
      "training classifier for 18 versus 8\n",
      "training classifier for 18 versus 9\n",
      "training classifier for 18 versus 10\n",
      "training classifier for 18 versus 11\n",
      "training classifier for 18 versus 12\n",
      "training classifier for 18 versus 13\n",
      "training classifier for 18 versus 14\n",
      "training classifier for 18 versus 15\n",
      "training classifier for 18 versus 16\n",
      "training classifier for 18 versus 17\n",
      "training classifier for 19 versus 0\n",
      "training classifier for 19 versus 1\n",
      "training classifier for 19 versus 2\n",
      "training classifier for 19 versus 3\n",
      "training classifier for 19 versus 4\n",
      "training classifier for 19 versus 5\n",
      "training classifier for 19 versus 6\n",
      "training classifier for 19 versus 7\n",
      "training classifier for 19 versus 8\n",
      "training classifier for 19 versus 9\n",
      "training classifier for 19 versus 10\n",
      "training classifier for 19 versus 11\n",
      "training classifier for 19 versus 12\n",
      "training classifier for 19 versus 13\n",
      "training classifier for 19 versus 14\n",
      "training classifier for 19 versus 15\n",
      "training classifier for 19 versus 16\n",
      "training classifier for 19 versus 17\n",
      "training classifier for 19 versus 18\n",
      "\n",
      "Full Wine Data confidence: 0.02504638218923933\n",
      "--- 13.239207983016968 seconds ---\n",
      "dark?\n",
      "-N-> tannins?\n",
      "|    -N-> up?\n",
      "|    |    -N-> class +1\t(5.0 for class -1, 59.0 for class +1)\n",
      "|    |    -Y-> class -1\t(6.0 for class -1, 1.0 for class +1)\n",
      "|    -Y-> class -1\t(7.0 for class -1, 0.0 for class +1)\n",
      "-Y-> class -1\t(29.0 for class -1, 0.0 for class +1)\n",
      "\"dark\" being present is most indicative of not being Shiraz\n",
      "\n",
      "\n",
      "C using AVA\n",
      "\n",
      "Full Wine Data zeroOne: 0.24304267161410018\n",
      "without zeroOne confidence = 0.027829313543599257 while zeroOne = 0.24768089053803338\n",
      "Just about a 2% difference in accuracy, which is negligible\n"
     ]
    }
   ],
   "source": [
    "# WU1 CODE HERE\n",
    "import time\n",
    "\n",
    "print(\"A using OVA\")\n",
    "h = multiclass.OVA(5, lambda: DecisionTreeClassifier(max_depth=3))\n",
    "h.train(WineDataSmall.X, WineDataSmall.Y)\n",
    "print(\"\\nSauvignon-Blanc\")\n",
    "util.showTree(h.f[0], WineDataSmall.words)\n",
    "print('\"citrus\" and \"grapefruit\" both being present combined with a lack of \"honey\"')\n",
    "print('are the most indicative of Sauvignon-Blanc')\n",
    "print(\"\\nCabernet-Sauvignon\")\n",
    "util.showTree(h.f[1], WineDataSmall.words)\n",
    "print('\"tannins\" and \"salmon\" are most indicative of not being Cabernet-Sauvignon')\n",
    "print(\"\\nPinot-Grigio\")\n",
    "util.showTree(h.f[4], WineDataSmall.words)\n",
    "print('\"apple\" \"straw\" and \"bright\" are most indicative of Pinot-Grigio')\n",
    "print('while the absense of \"apple\" \"paired\" and \"friends\" are most indicative of')\n",
    "print('not being Pinot-Grigio')\n",
    "\n",
    "\n",
    "print(\"\\n\\nB using OVA\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "h = multiclass.OVA(20, lambda: DecisionTreeClassifier(max_depth=3))\n",
    "h.train(WineData.X, WineData.Y)\n",
    "P = h.predictAll(WineData.Xte)\n",
    "print(\"\\nFull Wine Data confidence: \" + str(mean(P == WineData.Yte)))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# print(WineData.labels[8])\n",
    "util.showTree(h.f[8], WineData.words)\n",
    "print(\"\\nWe get a YTest accuracry of 37% (0.36641929499072357)\")\n",
    "print(\"It takes about 1.6 seconds to accomplish this\")\n",
    "\n",
    "print('\"characters\" \"dark\" and the absence of \"it\" are most indicative of being Shiraz')\n",
    "\n",
    "print('\"barossa\" and \"brooding\" are most indicative of not being Shiraz')\n",
    "\n",
    "print(\"\\n\\nC using OVA\")\n",
    "P = h.predictAll(WineData.Xte, True)\n",
    "print(\"\\nFull Wine Data zeroOne: \" + str(mean(P == WineData.Yte)))\n",
    "\n",
    "print(\"without zeroOne confidence = 0.3682745825602968 while zeroOne = 0.2504638218923933\")\n",
    "print(\"Just about a 10% difference in accuracy, which is sizeable\")\n",
    "\n",
    "\n",
    "print(\"\\n\\nA using AVA\")\n",
    "h = multiclass.AVA(5, lambda: DecisionTreeClassifier(max_depth=3))\n",
    "h.train(WineDataSmall.X, WineDataSmall.Y)\n",
    "print(\"\\nSauvignon-Blanc\")\n",
    "util.showTree(h.f[1][0], WineDataSmall.words)\n",
    "print('\\n\"Citrus\" is by far the most indicative of being Sauvignon-Blanc')\n",
    "print(\"\\nCabernet-Sauvignon\")\n",
    "util.showTree(h.f[2][0], WineDataSmall.words)\n",
    "print('\"crip\" \"red\" are most indicative of not being Cabernet-Sauvignon')\n",
    "print(\"\\nPinot-Grigio\")\n",
    "util.showTree(h.f[4][0], WineDataSmall.words)\n",
    "print('\"apple\" \"paste\" \"warm\" all being absent are most indicative of being Pinot-Grigio')\n",
    "print('\"apple\" being present and \"bright\" being absent are most indicative of not being Pinot-Grigio')\n",
    "\n",
    "\n",
    "print(\"\\n\\nB using AVA\")\n",
    "start_time = time.time()\n",
    "\n",
    "h = multiclass.AVA(20, lambda: DecisionTreeClassifier(max_depth=3))\n",
    "h.train(WineData.X, WineData.Y)\n",
    "P = h.predictAll(WineData.Xte)\n",
    "print(\"\\nFull Wine Data confidence: \" + str(mean(P == WineData.Yte)))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "util.showTree(h.f[8][0], WineData.words)\n",
    "print('\"dark\" being present is most indicative of not being Shiraz')\n",
    "\n",
    "\n",
    "print(\"\\n\\nC using AVA\")\n",
    "\n",
    "P = h.predictAll(WineData.Xte, True)\n",
    "print(\"\\nFull Wine Data zeroOne: \" + str(mean(P == WineData.Yte)))\n",
    "print(\"without zeroOne confidence = 0.027829313543599257 while zeroOne = 0.24768089053803338\")\n",
    "print(\"Just about a 2% difference in accuracy, which is negligible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[WU1 ANSWER HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/kitten.jpeg\" width=\"100px\" align=\"left\" float=\"left\"/>\n",
    "<br><br><br>\n",
    "\n",
    "## WU2 (10%):\n",
    "Using decision trees of constant depth for each\n",
    "classifier (but you choose it as well as you can!), train AVA, OVA and\n",
    "MCTree (using balanced trees) for the WineDataSmall. Which one produces best recall and why? Answer the same for WineData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVA error bound = 2(K-1)e\n",
      "OVA error bound = (K-1)e\n",
      "MCTree error bound = (log_2 K)e\n",
      "As the error bound of the MCTree scales logarithmically with K, it is natural that the confidence level is better given the same K\n",
      "This applies both to WineDataSmall and WineData.  In fact the size of the dataset doesn't much affect the confidence level after a point, with K having much stronger influence\n",
      "\n",
      "\n",
      "Training Small Wine Data OVA using depth=5\n",
      "training classifier for 0 versus rest\n",
      "training classifier for 1 versus rest\n",
      "training classifier for 2 versus rest\n",
      "training classifier for 3 versus rest\n",
      "training classifier for 4 versus rest\n",
      "Small Wine Data OVA confidence: 0.6017505470459519\n",
      "\n",
      "\n",
      "Training Small Wine Data AVA using depth=5\n",
      "training classifier for 1 versus 0\n",
      "training classifier for 2 versus 0\n",
      "training classifier for 2 versus 1\n",
      "training classifier for 3 versus 0\n",
      "training classifier for 3 versus 1\n",
      "training classifier for 3 versus 2\n",
      "training classifier for 4 versus 0\n",
      "training classifier for 4 versus 1\n",
      "training classifier for 4 versus 2\n",
      "training classifier for 4 versus 3\n",
      "Small Wine Data AVA confidence: 0.024070021881838075\n",
      "\n",
      "\n",
      "Training Small Wine Data MCTree using depth=5\n",
      "training classifier for [0, 1] versus [2, 3, 4]\n",
      "training classifier for [0] versus [1]\n",
      "training classifier for [2] versus [3, 4]\n",
      "training classifier for [3] versus [4]\n",
      "Small Wine Data MCTree confidence: 0.0787746170678337\n",
      "Which one produces best recall and why? Answer the same for WineData.\n",
      "On Small Wine Data MCTree produces the best confidence\n",
      "\n",
      "\n",
      "Training Full Wine Data OVA using depth=5\n",
      "training classifier for 0 versus rest\n",
      "training classifier for 1 versus rest\n",
      "training classifier for 2 versus rest\n",
      "training classifier for 3 versus rest\n",
      "training classifier for 4 versus rest\n",
      "Full Wine Data OVA confidence: 0.22634508348794063\n",
      "\n",
      "Training Full Wine Data AVA using depth=5\n",
      "training classifier for 1 versus 0\n",
      "training classifier for 2 versus 0\n",
      "training classifier for 2 versus 1\n",
      "training classifier for 3 versus 0\n",
      "training classifier for 3 versus 1\n",
      "training classifier for 3 versus 2\n",
      "training classifier for 4 versus 0\n",
      "training classifier for 4 versus 1\n",
      "training classifier for 4 versus 2\n",
      "training classifier for 4 versus 3\n",
      "Full Wine Data AVA confidence: 0.023191094619666047\n",
      "\n",
      "Training Full Wine Data MCTree using depth=5\n",
      "training classifier for [0, 1] versus [2, 3, 4]\n",
      "training classifier for [0] versus [1]\n",
      "training classifier for [2] versus [3, 4]\n",
      "training classifier for [3] versus [4]\n",
      "Full Wine Data MCTree confidence: 0.061224489795918366\n"
     ]
    }
   ],
   "source": [
    "# WU2 CODE HERE\n",
    "\n",
    "print(\"AVA error bound = 2(K-1)e\")\n",
    "print(\"OVA error bound = (K-1)e\")\n",
    "print(\"MCTree error bound = (log_2 K)e\")\n",
    "print(\"As the error bound of the MCTree scales logarithmically with K, it is natural that the confidence level is better given the same K\")\n",
    "print(\"This applies both to WineDataSmall and WineData.  In fact the size of the dataset doesn't much affect the confidence level after a point, with K having much stronger influence\")\n",
    "\n",
    "print(\"\\n\\nTraining Small Wine Data OVA using depth=5\")\n",
    "h = multiclass.OVA(5, lambda: DecisionTreeClassifier(max_depth=5))\n",
    "h.train(WineDataSmall.X, WineDataSmall.Y)\n",
    "P = h.predictAll(WineDataSmall.Xte)\n",
    "print(\"Small Wine Data OVA confidence: \" + str(np.mean(P == WineDataSmall.Yte)))\n",
    "\n",
    "print(\"\\n\\nTraining Small Wine Data AVA using depth=5\")\n",
    "h = multiclass.AVA(5, lambda: DecisionTreeClassifier(max_depth=5))\n",
    "h.train(WineDataSmall.X, WineDataSmall.Y)\n",
    "P = h.predictAll(WineDataSmall.Xte)\n",
    "print(\"Small Wine Data AVA confidence: \" + str(np.mean(P == WineDataSmall.Yte)))\n",
    "\n",
    "print(\"\\n\\nTraining Small Wine Data MCTree using depth=5\")\n",
    "t = multiclass.makeBalancedTree(range(5))\n",
    "h = multiclass.MCTree(t, lambda: DecisionTreeClassifier(max_depth=5))\n",
    "h.train(WineDataSmall.X, WineDataSmall.Y)\n",
    "P = h.predictAll(WineDataSmall.Xte)\n",
    "print(\"Small Wine Data MCTree confidence: \" + str(np.mean(P == WineDataSmall.Yte)))\n",
    "\n",
    "print(\"Which one produces best recall and why? Answer the same for WineData.\")\n",
    "print(\"On Small Wine Data MCTree produces the best confidence\")\n",
    "\n",
    "\n",
    "print(\"\\n\\nTraining Full Wine Data OVA using depth=5\")\n",
    "h = multiclass.OVA(5, lambda: DecisionTreeClassifier(max_depth=5))\n",
    "h.train(WineData.X, WineData.Y)\n",
    "P = h.predictAll(WineData.Xte)\n",
    "print(\"Full Wine Data OVA confidence: \" + str(np.mean(P == WineData.Yte)))\n",
    "\n",
    "print(\"\\nTraining Full Wine Data AVA using depth=5\")\n",
    "h = multiclass.AVA(5, lambda: DecisionTreeClassifier(max_depth=5))\n",
    "h.train(WineData.X, WineData.Y)\n",
    "P = h.predictAll(WineData.Xte)\n",
    "print(\"Full Wine Data AVA confidence: \" + str(np.mean(P == WineData.Yte)))\n",
    "\n",
    "print(\"\\nTraining Full Wine Data MCTree using depth=5\")\n",
    "t = multiclass.makeBalancedTree(range(5))\n",
    "h = multiclass.MCTree(t, lambda: DecisionTreeClassifier(max_depth=5))\n",
    "h.train(WineData.X, WineData.Y)\n",
    "P = h.predictAll(WineData.Xte)\n",
    "print(\"Full Wine Data MCTree confidence: \" + str(np.mean(P == WineData.Yte)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[WU2 ANSWER HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/kitten.jpeg\" width=\"100px\" align=\"left\" float=\"left\"/>\n",
    "<br><br><br>\n",
    "\n",
    "## WU-EC1 ExtraCredit (10%):\n",
    "Build a better tree (any way you want) other\n",
    "than the balanced binary tree. Fill in your code for this in\n",
    "`getMyTreeForWine`, which defaults to a balanced tree. It should get\n",
    "at least 5% lower absolute error to get the extra credit. Describe what you\n",
    "did.\n",
    "\n",
    "[YOUR WU-EC1 ANSWER HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Gradient Descent and Linear Classification *[30% impl, 20% writeup]*\n",
    "\n",
    "To get started with linear models, we will implement a generic\n",
    "gradient descent method.  This should go in `gd.py`, which\n",
    "contains a single (short) function: `gd`. This takes five\n",
    "parameters: the function we're optimizing, it's gradient, an initial\n",
    "position, a number of iterations to run, and an initial step size.\n",
    "\n",
    "In each iteration of gradient descent, we will compute the gradient\n",
    "and take a step in that direction, with step size `eta`.  We\n",
    "will have an *adaptive* step size, where `eta` is computed\n",
    "as `stepSize` divided by the square root of the iteration\n",
    "number (counting from one).\n",
    "\n",
    "Once you have an implementation running, we can check it on a simple\n",
    "example of minimizing the function `x^2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0034641051795872,\n",
       " array([100.        ,  36.        ,  18.5153247 ,  10.95094653,\n",
       "          7.00860578,   4.72540613,   3.30810578,   2.38344246,\n",
       "          1.75697198,   1.31968118,   1.00694021]))"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gd\n",
    "gd.gd(lambda x: x**2, lambda x: 2*x, 10, 10, 0.2)\n",
    "#(1.0034641051795872, array([ 100.        ,   36.        ,   18.5153247 ,   10.95094653,\n",
    "#          7.00860578,    4.72540613,    3.30810578,    2.38344246,\n",
    "#          1.75697198,    1.31968118,    1.00694021]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the \"solution\" found is about 1, which is not great\n",
    "(it should be zero!), but it's better than the initial value of ten!\n",
    "If yours is going up rather than going down, you probably have a sign\n",
    "error somewhere!\n",
    "\n",
    "We can let it run longer and plot the trajectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0841010700891128e+21\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x, trajectory = gd.gd(lambda x: x**2, lambda x: 2*x, 10, 50, 8)\n",
    "print(x)\n",
    "# 0.003645900464603937\n",
    "plt.plot(trajectory)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now found a value close to zero and you can see that the\n",
    "objective is decreasing by looking at the plot.\n",
    "\n",
    "<img src=\"data/kitten.jpeg\" width=\"100px\" align=\"left\" float=\"left\"/>\n",
    "<br><br><br>\n",
    "\n",
    "## WU3 (5%):\n",
    "Find a few values of step size where it converges and\n",
    "a few values where it diverges.  Where does the threshold seem to\n",
    "be?\n",
    "\n",
    "[Your WU3 answer here]\n",
    "\n",
    "The solution converges to 0.0 at a limit of 0.5 (step size)\n",
    "\n",
    "The solution diverges to positive infinity at step sizes above 10, it is related to the numIters, as halving the value to 50 causes divergence to infinity at stepSize=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/kitten.jpeg\" width=\"100px\" align=\"left\" float=\"left\"/>\n",
    "<br><br><br>\n",
    "\n",
    "## WU4 (10%):\n",
    "Come up with a *non-convex* univariate\n",
    "optimization problem.  Plot the function you're trying to minimize and\n",
    "show two runs of `gd`, one where it gets caught in a local\n",
    "minimum and one where it manages to make it to a global minimum.  (Use\n",
    "different starting points to accomplish this.)\n",
    "\n",
    "If you implemented it well, this should work in multiple dimensions,\n",
    "too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0036459  0.00182295]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, trajectory = gd.gd(lambda x: linalg.norm(x)**2, lambda x: 2*x, array([10,5]), 100, 0.2)\n",
    "print(x)\n",
    "# array([ 0.0036459 ,  0.00182295])\n",
    "plt.plot(trajectory)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our generic linear classifier implementation is\n",
    "in `linear.py`.  The way this works is as follows.  We have an\n",
    "interface `LossFunction` that we want to minimize.  This must\n",
    "be able to compute the loss for a pair `Y` and `Yhat`\n",
    "where, the former is the truth and the latter are the predictions.  It\n",
    "must also be able to compute a gradient when additionally given the\n",
    "data `X`.  This should be all you need for these.\n",
    "\n",
    "There are two loss function stubs: `SquaredLoss` (which is\n",
    "implemented for you!), `LogisticLoss`\n",
    "(you'll have to implement.  My suggestion is to hold off\n",
    "implementing it until you have the linear classifier\n",
    "working.\n",
    "\n",
    "The `LinearClassifier` class is a stub implemention of a\n",
    "generic linear classifier with an l2 regularizer.  It\n",
    "is *unbiased* so all you have to take care of are the weights.\n",
    "Your implementation should go in `train`, which has a handful\n",
    "of stubs.  The idea is to just pass appropriate functions\n",
    "to `gd` and have it do all the work.  See the comments inline\n",
    "in the code for more information.\n",
    " \n",
    "Once you've implemented the function evaluation and gradient, we can\n",
    "test this.  We'll begin with a very simple 2D example data set so that\n",
    "we can plot the solutions.  We'll also start with *no\n",
    "regularizer* to help you figure out where errors might be if you\n",
    "have them.  (You'll have to import `mlGraphics` to make this\n",
    "work.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 0.9, test accuracy 0.9\n",
      "w=array([232.37004781,   0.        ])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertdavidhernandez/Documents/grad school/UIC/CS412 Intro ML/hw3/mlGraphics.py:40: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  xmin_zero = - (b + w[0] * xmin) / w[1]\n",
      "/Users/robertdavidhernandez/Documents/grad school/UIC/CS412 Intro ML/hw3/mlGraphics.py:41: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  xmax_zero = - (b + w[0] * xmax) / w[1]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import linear\n",
    "import datasets\n",
    "import mlGraphics\n",
    "import runClassifier\n",
    "f = linear.LinearClassifier({'lossFunction': linear.SquaredLoss(), 'lambda': 0, 'numIter': 100, 'stepSize': 0.5})\n",
    "runClassifier.trainTestSet(f, datasets.TwoDAxisAligned)\n",
    "\n",
    "# Training accuracy 0.91, test accuracy 0.86\n",
    "print(f)\n",
    "# w=array([ 2.73466371, -0.29563932])\n",
    "mlGraphics.plotLinearClassifier(f, datasets.TwoDAxisAligned.X, datasets.TwoDAxisAligned.Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even though this data is clearly linearly separable,\n",
    "the *unbiased* classifier is unable to perfectly separate it.\n",
    "\n",
    "If we change the regularizer, we'll get a slightly different\n",
    "solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 0.9, test accuracy 0.9\n",
      "w=array([2.5, 0. ])\n"
     ]
    }
   ],
   "source": [
    "f = linear.LinearClassifier({'lossFunction': linear.SquaredLoss(), 'lambda': 10, 'numIter': 100, 'stepSize': 0.5})\n",
    "runClassifier.trainTestSet(f, datasets.TwoDAxisAligned)\n",
    "# Training accuracy 0.9, test accuracy 0.86\n",
    "print(f)\n",
    "# w=array([ 1.30221546, -0.06764756])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the weights are *smaller*.\n",
    "\n",
    "Now, we can try different loss function.  Implement logistic loss.  Here are some simple test cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 0.99, test accuracy 0.86\n",
      "w=array([0.78, 2.5 ])\n"
     ]
    }
   ],
   "source": [
    "f = linear.LinearClassifier({'lossFunction': linear.LogisticLoss(), 'lambda': 10, 'numIter': 100, 'stepSize': 0.5})\n",
    "runClassifier.trainTestSet(f, datasets.TwoDDiagonal)\n",
    "# Training accuracy 0.99, test accuracy 0.86\n",
    "print(f)\n",
    "# w=array([ 0.29809083,  1.01287561])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/kitten.jpeg\" width=\"100px\" align=\"left\" float=\"left\"/>\n",
    "<br><br><br>\n",
    "\n",
    "## WU5 (5%):\n",
    "For each of the loss functions, train a model on the\n",
    "binary version of the wine data (called WineDataBinary) and evaluate\n",
    "it on the test data. You should use lambda=2 in all cases. Which works\n",
    "best? For that best model, look at the learned weights. Find\n",
    "the *words* corresponding to the weights with the greatest\n",
    "positive value and those with the greatest negative value (this is\n",
    "like [LAB3]). Hint: look at WineDataBinary.words to get the id-to-word\n",
    "mapping. List the top 5 positive and top 5 negative and explain.\n",
    "\n",
    "[Your WU5 answer here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfortunately I couldn't get the axis to line up\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (819,) (2,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-487-010901811fbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'lossFunction'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSquaredLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lambda'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'numIter'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stepSize'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrunClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainTestSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWineDataBinary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/grad school/UIC/CS412 Intro ML/hw3/runClassifier.py\u001b[0m in \u001b[0;36mtrainTestSet\u001b[0;34m(classifier, dataset)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrainTestSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mtrainTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mYte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlearningCurve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/grad school/UIC/CS412 Intro ML/hw3/runClassifier.py\u001b[0m in \u001b[0;36mtrainTest\u001b[0;34m(classifier, X, Y, Xtest, Ytest)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                           \u001b[0;31m# initialize the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m                      \u001b[0;31m# train it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m#print \"Learned Classifier:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/grad school/UIC/CS412 Intro ML/hw3/linear.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;31m# run gradient descent; our initial point will just be our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;31m# weight vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrajectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumIter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstepSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# store the weights and trajectory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/grad school/UIC/CS412 Intro ML/hw3/gd.py\u001b[0m in \u001b[0;36mgd\u001b[0;34m(func, grad, x0, numIter, stepSize)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumIter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# compute the gradient at the current location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# compute the step size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/grad school/UIC/CS412 Intro ML/hw3/linear.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mYhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mgr\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mlossFn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlossGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYhat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambd\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (819,) (2,) "
     ]
    }
   ],
   "source": [
    "print(\"Unfortunately I couldn't get the axis to line up\")\n",
    "\n",
    "f = linear.LinearClassifier({'lossFunction': linear.SquaredLoss(), 'lambda': 2, 'numIter': 100, 'stepSize': 0.5})\n",
    "runClassifier.trainTestSet(f, datasets.WineDataBinary)\n",
    "print(f)\n",
    "\n",
    "f = linear.LinearClassifier({'lossFunction': linear.LogisticLoss(), 'lambda': 2, 'numIter': 100, 'stepSize': 0.5})\n",
    "runClassifier.trainTestSet(f, datasets.WineDataBinary)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Classification with Many Classes *[0% -- up to 15% extra credit]*\n",
    "\n",
    "Finally, we'll do multiclass classification **using Scikit-learn** functionality. You can find the documentation here: http://scikit-learn.org/stable/modules/multiclass.html.\n",
    "\n",
    "Quiz bowl is a game in which two teams compete head-to-head to answer questions from different areas of knowledge. It lets players interrupt the reading of a question when they know the answer. The goal here is to see how well a classifier performs in predicting the `Answer` of a question when a different portion of the question is revealed.\n",
    "\n",
    "Here's an example question from the development data:\n",
    "\n",
    "*206824,dev,History,Alan Turing,\"This man and Donald Bayley created a secure voice communications machine called \"\"Delilah\"\". ||| The Chinese Room Experiment was developed by John Searle in response to one of this man's namesake tests. ||| He showed that the halting problem was undecidable. ||| He devised a bomb with Gordon Welchman that found the settings of an Enigma machine. ||| One of this man's eponymous machines which can perform any computing task is his namesake \"\"complete.\"\" Name this man, whose eponymous test is used to determine if a machine can exhibit behavior indistinguishable from that of a human.\"*\n",
    "\n",
    "The more of the question you get, the easier the problem becomes.\n",
    "\n",
    "The default code below just runs OVA and AVA on top of a linear SVM (it might take a few seconds):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Quizbowl dataset..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total labels: 2370\n",
      "unique features: 8416\n",
      "total training examples: 8845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading QuizbowlSmall dataset..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total labels: 31\n",
      "unique features: 8416\n",
      "total training examples: 361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading QuizbowlHard dataset..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total labels: 2370\n",
      "unique features: 4132\n",
      "total training examples: 8845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading QuizbowlHardSmall dataset..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total labels: 31\n",
      "unique features: 4132\n",
      "total training examples: 361\n",
      "\n",
      "\n",
      "RUNNING ON EASY DATA\n",
      "\n",
      "training ova\n",
      "predicting ova\n",
      "error = 0.2934131736526946\n",
      "training ava\n",
      "predicting ava\n",
      "error = 0.218562874251497\n",
      "\n",
      "\n",
      "RUNNING ON HARD DATA\n",
      "\n",
      "training ova\n",
      "predicting ova\n",
      "error = 0.5958083832335329\n",
      "training ava\n",
      "predicting ava\n",
      "error = 0.5538922155688623\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from numpy import *\n",
    "import datasets\n",
    "import importlib\n",
    "\n",
    "importlib.reload(datasets)\n",
    "\n",
    "if not datasets.Quizbowl.loaded:\n",
    "    datasets.loadQuizbowl()\n",
    "\n",
    "print('\\n\\nRUNNING ON EASY DATA\\n')\n",
    "    \n",
    "print('training ova')\n",
    "X = datasets.QuizbowlSmall.X\n",
    "Y = datasets.QuizbowlSmall.Y\n",
    "ova = OneVsOneClassifier(LinearSVC(random_state=0)).fit(X, Y)\n",
    "print('predicting ova')\n",
    "ovaDevPred = ova.predict(datasets.QuizbowlSmall.Xde)\n",
    "print('error = {0}'.format(mean(ovaDevPred != datasets.QuizbowlSmall.Yde)))\n",
    "\n",
    "print('training ava')\n",
    "ava = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X, Y)\n",
    "print('predicting ava')\n",
    "avaDevPred = ava.predict(datasets.QuizbowlSmall.Xde)\n",
    "print('error = {0}'.format(mean(avaDevPred != datasets.QuizbowlSmall.Yde)))\n",
    "\n",
    "print('\\n\\nRUNNING ON HARD DATA\\n')\n",
    "    \n",
    "print('training ova')\n",
    "X = datasets.QuizbowlHardSmall.X\n",
    "Y = datasets.QuizbowlHardSmall.Y\n",
    "ova = OneVsOneClassifier(LinearSVC(random_state=0)).fit(X, Y)\n",
    "print('predicting ova')\n",
    "ovaDevPred = ova.predict(datasets.QuizbowlHardSmall.Xde)\n",
    "print('error = {0}'.format(mean(ovaDevPred != datasets.QuizbowlHardSmall.Yde)))\n",
    "\n",
    "print('training ava')\n",
    "ava = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X, Y)\n",
    "print('predicting ava')\n",
    "avaDevPred = ava.predict(datasets.QuizbowlHardSmall.Xde)\n",
    "print('error = {0}'.format(mean(avaDevPred != datasets.QuizbowlHardSmall.Yde)))\n",
    "\n",
    "savetxt('predictions.txt', avaDevPred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run the code above, you should see some statistics of the loaded datasets and the following error rates on two of the datasets `QuizbowlSmall` and `QuizbowlHardSmall` using OVA and AVA:\n",
    "\n",
    "```\n",
    "RUNNING ON EASY DATA\n",
    "\n",
    "training ova\n",
    "predicting ova\n",
    "error = 0.293413\n",
    "training ava\n",
    "predicting ava\n",
    "error = 0.218563\n",
    "\n",
    "\n",
    "RUNNING ON HARD DATA\n",
    "\n",
    "training ova\n",
    "predicting ova\n",
    "error = 0.595808\n",
    "training ava\n",
    "predicting ava\n",
    "error = 0.553892\n",
    "```\n",
    "\n",
    "This is running on a shrunken version of the data (that only contains answers that occur at least 20 times in the data).\n",
    "\n",
    "The first (\"easy\") version is when you get to see the entire question. The second (\"hard\") version is when you only get to use the first two sentences. It's clearly significantly harder to answer!\n",
    "\n",
    "<img src=\"data/kitten.jpeg\" width=\"100px\" align=\"left\" float=\"left\"/>\n",
    "<br><br><br>\n",
    "\n",
    "## WU-EC2 (5%):\n",
    "\n",
    "Your task is to achieve the lowest possible error on the development set for `QuizbowlSmall` and `QuizbowlHardSmall`. You will get 5% extra credit for getting lower error (by at least absolute 1%) on *either* dataset than the errors presented above (21.86% for `QuizbowlSmall` and 55.39% for `QuizbowlHardSmall`). \n",
    "\n",
    "You're free to use the training data in any way you want, but you must include your code in `quizbowl.py`, submit your predictions file(s), and a writeup here that says what you did, in order to receive the extra credit. The script `quizbowl.py` includes a command in the last line that saves predictions to a text file `predictions.txt`. You need to edit this line to rename the file to either `predictionsQuizbowlSmall.txt` or `predictionsQuizbowlHardSmall.txt` dependent on the dataset: that's what you upload for the EC. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[YOUR WU-EC2 WRITEUP HERE] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/kitten.jpeg\" width=\"100px\" align=\"left\" float=\"left\"/>\n",
    "<br><br><br>\n",
    "\n",
    "## WU-EC3 (up to 10%):\n",
    "\n",
    "\n",
    "Additionally, you can get extra credit for providing the lowest-error solution on the full versions of the easy and hard problems, `Quizbowl` and `QuizbowlHard` in comparison to your classmates' solutions. There will be two separate (hidden) leaderboards for the two respective datasets. You will receive 5% if your solution is the best for the respective dataset (first place), 3% for second place and 1% for third. You are welcome to compete in both leaderboards which lets you earn up to 10% (5 + 5) extra credits for securing first place in both. We will reveal the top three scores for each dataset after the submission period is over. Note that this problem is much harder due to the larger number of class labels. A simple majority label classifier has an error of 99.89%.\n",
    "\n",
    "You're free to use the training data in any way you want, but you must include your code in `quizbowl.py`, submit your predictions file(s) (`predictionsQuizbowl.txt` and/or `predictionsQuizbowlHard.txt`), and a writeup here that says what you did, in order to receive the extra credit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[YOUR WU-EC3 WRITEUP HERE] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
